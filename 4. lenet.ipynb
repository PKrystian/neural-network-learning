{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05de5415",
   "metadata": {},
   "source": [
    "# LeNet-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e1492b",
   "metadata": {},
   "source": [
    "**Korzystając z notebooka o sieciach konwolucyjnych oraz notebooka o płytkiej sieci w Tensorflow, za pomocą biblioteki Tensorflow zaprojektuj konwolucyjną sieć neuronową o architekturze zbliżonej do słynnej sieci [LeNet-5](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) której sukcesy w rozpoznawaniu obrazów rozpoczęły na przełomie lat 80. i 90. boom na głębokie uczenie, i użyj jej do predykcji zbioru FashionMnist, zgodnie z następującym opisem:**\n",
    "\n",
    "|Nr warstwy|Rodzaj|Rozmiar wyjścia|Liczba filtrów|Rozmiar jądra (`kernel_size`)|Krok (`stride`)|Padding|Funkcja aktywacji|\n",
    "|----------|---|---------------|--------------|-----------------------|---------------|-------|-----------------|\n",
    "|1|splotowa|$28\\times 28$|6|$5\\times 5$|1|2|tanh|\n",
    "|2|avgerage pooling|$14\\times 14$|6|$2\\times 2$|2|0|sigmoid|\n",
    "|3|splotowa|$10\\times 10$|16|$5\\times 5$|1|0|tanh|\n",
    "|4|average pooling|$5\\times 5$|16|$2\\times 2$|2|0|sigmoid|\n",
    "|5|splotowa|$1\\times 1$|120|$5\\times 5$|1|0|tanh|\n",
    "|6|pełna|84|||||tanh|\n",
    "|7|pełna|10|||||softmax|\n",
    "\n",
    "**Kolejne kroki:**\n",
    " - załadowanie zbioru\n",
    " - wydzielenie zbioru walidacyjnego\n",
    " - normalizacja i kategoryzacja danych (pomiędzy normalizacją i kategoryzacją przekształć zarówno zbiór treningowy jak i walidacyjny w tensory o odpowiednich wymiarach za pomocą funkcji _expand_dims(dataset, axis=3)_)\n",
    " - zaprojektowanie sieci (nie zapominaj o spłaszczeniu w odpowiednim momencie)\n",
    " - zbudowanie modelu z dowolną funkcją kosztu dla problemów wieloklasowych, dowolnym optymalizatorem i trafnością jako metryką\n",
    " - nauczenie sieci w maksymalnie stu epokach z zastosowaniem wczesnego zatrzymania (najlepiej użyć wbudowanego, z dowolną wartością do monitorowania)\n",
    " - ewaluacja modelu na zbiorze walidacyjnym\n",
    " - wyświetlenie 10 niepoprawnie zaklasyfikowanych próbek w postaci: prawdziwa prognozowana, np. 2 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fe87dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalize and categorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "test_images = np.expand_dims(test_images, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert labels to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(6, (5, 5), strides=(1, 1), padding='same', activation='tanh', input_shape=(28, 28, 1)))\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "model.add(layers.Conv2D(16, (5, 5), strides=(1, 1), padding='valid', activation='tanh'))\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "model.add(layers.Conv2D(120, (5, 5), strides=(1, 1), padding='valid', activation='tanh'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(84, activation='tanh'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training, Evaluation, Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 6s 3ms/step - loss: 1.0360 - accuracy: 0.5998\n",
      "313/313 - 1s - loss: 0.6844 - accuracy: 0.7435 - 542ms/epoch - 2ms/step\n",
      "Epoch 1/100 Test loss: 0.6844043731689453 Test accuracy: 0.7434999942779541\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6534 - accuracy: 0.7464\n",
      "313/313 - 0s - loss: 0.6562 - accuracy: 0.7491 - 458ms/epoch - 1ms/step\n",
      "Epoch 2/100 Test loss: 0.6561905145645142 Test accuracy: 0.7491000294685364\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5900 - accuracy: 0.7712\n",
      "313/313 - 0s - loss: 0.5693 - accuracy: 0.7906 - 474ms/epoch - 2ms/step\n",
      "Epoch 3/100 Test loss: 0.569276750087738 Test accuracy: 0.7906000018119812\n",
      " 352/1875 [====>.........................] - ETA: 4s - loss: 0.5503 - accuracy: 0.7859"
     ]
    }
   ],
   "source": [
    "PATIENCE = 5\n",
    "WAIT = 0\n",
    "EPOCHS_NUM = 100\n",
    "\n",
    "for epoch in range(EPOCHS_NUM):\n",
    "    model.fit(train_images, train_labels, epochs=1)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        WAIT = 0\n",
    "    else:\n",
    "        WAIT += 1\n",
    "        if WAIT >= PATIENCE:\n",
    "            print(f\"Early stopping on epoch {epoch}\")\n",
    "            break\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_NUM} Test loss: {test_loss} Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_images)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ten wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 9, Predicted: 7\n",
      "Actual: 6, Predicted: 2\n",
      "Actual: 4, Predicted: 2\n",
      "Actual: 6, Predicted: 4\n",
      "Actual: 4, Predicted: 6\n",
      "Actual: 5, Predicted: 7\n",
      "Actual: 9, Predicted: 5\n",
      "Actual: 4, Predicted: 6\n",
      "Actual: 6, Predicted: 4\n",
      "Actual: 0, Predicted: 3\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "actual_labels = np.argmax(test_labels, axis=1)\n",
    "wrong_indices = []\n",
    "for i in range(len(actual_labels)):\n",
    "    actual = actual_labels[i]\n",
    "    pred = predicted_labels[i]\n",
    "    if actual != pred:\n",
    "        wrong_indices.append(i)\n",
    "for i in wrong_indices[:10]:\n",
    "    print(f\"Actual: {actual_labels[i]}, Predicted: {predicted_labels[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
